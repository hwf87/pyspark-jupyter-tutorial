{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a40b577-91f0-488c-aad8-6573df56e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa9ac0-8463-4b90-a433-978bed043728",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9c3fcd-8b39-4de7-a7b9-a830a1bdd6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext: <SparkContext master=local appName=Python Spark Example>\n",
      "SparkSession: <pyspark.sql.session.SparkSession object at 0xffff626ff970>\n"
     ]
    }
   ],
   "source": [
    "# sc & ss\n",
    "AppName, Mode = \"Python Spark Example\", \"local\"\n",
    "conf = SparkConf().setAppName(AppName).setMaster(Mode)\n",
    "sc = SparkContext(conf=conf)\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "print(f\"SparkContext: {sc}\")\n",
    "print(f\"SparkSession: {ss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2688955-a584-40db-b471-f7138613327b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab7f4d-b030-4134-afa7-a3a3e7c1eb1b",
   "metadata": {},
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94f03e3-8f01-4fb0-865b-d9d081f27de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pyspark.rdd.RDD'>, Data: [[1, 2, 3], [4, 5], [6], [7, 8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# with Parallelized Collections\n",
    "data = [\n",
    "    [1, 2, 3], \n",
    "    [4, 5],\n",
    "    [6],\n",
    "    [7, 8, 9, 10]\n",
    "]\n",
    "persist_data = sc.parallelize(data).persist()\n",
    "print(f\"Type: {type(persist_data)}, Data: {persist_data.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ffe14-7308-4def-a5d0-c53e5299e7e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e42edc-26d2-4c8a-ae2f-6dcce7d72672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " first: [1, 2, 3]\n",
      " take: [[1, 2, 3], [4, 5]]\n",
      " count: 4\n",
      " collect: [[1, 2, 3], [4, 5], [6], [7, 8, 9, 10]]\n",
      " countByKey: defaultdict(<class 'int'>, {1: 1, 4: 1, 6: 1, 7: 1})\n"
     ]
    }
   ],
   "source": [
    "# Basic Operation \n",
    "print(f\" first: {persist_data.first()}\")\n",
    "print(f\" take: {persist_data.take(2)}\")\n",
    "print(f\" count: {persist_data.count()}\")\n",
    "print(f\" collect: {persist_data.collect()}\")\n",
    "print(f\" countByKey: {persist_data.countByKey()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7d4a38-8bff-4edf-99f1-f80242763928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result: [[2, 3, 4], [5, 6], [7], [8, 9, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "result = persist_data.map(lambda x: [i+1 for i in x])\n",
    "print(f\" Result: {result.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5032d9d-9712-4e57-8273-188a031b9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "result = persist_data.flatMap(lambda x: [i+1 for i in x])\n",
    "print(f\" Result: {result.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789f3149-a260-41a7-8380-c1805a3bd115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result: [[7, 8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "result = persist_data.filter(lambda x: 7 in x)\n",
    "print(f\" Result: {result.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac86eda-cf12-46b1-9b68-2782dbf01abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result_1: 10\n",
      " Result_2: 28\n"
     ]
    }
   ],
   "source": [
    "# foreach: Run a function func on each element of the dataset. \n",
    "# This is usually done for side effects such as updating an Accumulator or interacting with external storage systems. \n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "class MyAccumulator:\n",
    "    def AccumulateByLength(nums: List[int]) -> None:\n",
    "        target = len(nums)\n",
    "        accum.add(target)\n",
    "    def AccumulateByFirstNum(nums: List[int]) -> None:\n",
    "        target = nums[0]\n",
    "        accum.add(target)\n",
    "\n",
    "persist_data.foreach(MyAccumulator.AccumulateByLength)  # 3+2+1+4 = 10\n",
    "print(f\" Result_1: {accum.value}\") #Accessed by driver\n",
    "\n",
    "persist_data.foreach(MyAccumulator.AccumulateByFirstNum) # 10+ (1+4+6+7) = 28\n",
    "print(f\" Result_2: {accum.value}\") #Accessed by driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6273243-2002-47aa-9e83-d4a17f134b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result_1: [[2, 3, 4], [5, 6], [7], [8, 9, 10, 11]]\n",
      " Result_2: [[0, 1, 2], [3, 4], [5], [6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# map: Return a new distributed dataset formed by passing each element of the source through a function func.\n",
    "# Passing Functions to Spark\n",
    "\n",
    "class MyComputer:\n",
    "    def plusone(nums: List[int]) -> list:\n",
    "        return [i+1 for i in nums]\n",
    "    def minusone(nums: List[int]) -> list:\n",
    "        return [i-1 for i in nums]\n",
    "\n",
    "result_1 = persist_data.map(MyComputer.plusone)\n",
    "result_2 = persist_data.map(MyComputer.minusone)\n",
    "print(f\" Result_1: {result_1.collect()}\")\n",
    "print(f\" Result_2: {result_2.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa86f57-6ea6-4b0b-bd33-92f5a98a2650",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab78b8a-05ec-4018-8179-f35ee8605923",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa9eb97-09ea-4a89-bd64-b4130a9d69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare json data\n",
    "people_json = [\n",
    "    {\"Name\": \"Jacky\", \"Age\": 27, \"Gender\": \"Male\", \"Country\": \"Taiwan\"},\n",
    "    {\"Name\": \"John\", \"Age\": 32, \"Gender\": \"Male\", \"Country\": \"Taiwan\"},\n",
    "    {\"Name\": \"Tom\", \"Age\": 42, \"Gender\": \"Male\", \"Country\": \"Japan\"},\n",
    "    {\"Name\": \"Mark\", \"Age\": 16, \"Gender\": \"Male\", \"Country\": \"Taiwan\"},\n",
    "    {\"Name\": \"Sandy\", \"Age\": 23, \"Gender\": \"Female\", \"Country\": \"US\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ce6ee4-6fa0-442b-8c24-b61e69d8af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+---+-------+------+-----+\n",
      "|Age|Country|Gender| Name|\n",
      "+---+-------+------+-----+\n",
      "| 27| Taiwan|  Male|Jacky|\n",
      "| 32| Taiwan|  Male| John|\n",
      "| 42|  Japan|  Male|  Tom|\n",
      "| 16| Taiwan|  Male| Mark|\n",
      "| 23|     US|Female|Sandy|\n",
      "+---+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use rdd.toDF() to create a spark DataFrame\n",
    "people_json = sc.parallelize(people_json)\n",
    "sample_df = people_json.toDF()\n",
    "sample_df.printSchema()\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409dee16-33bd-4f74-b6c8-48b54226aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-----+---+------+-------+\n",
      "|Name |Age|Gender|Country|\n",
      "+-----+---+------+-------+\n",
      "|Jacky|27 |Male  |Taiwan |\n",
      "|John |32 |Male  |Taiwan |\n",
      "|Tom  |42 |Male  |Japan  |\n",
      "|Mark |16 |Male  |Taiwan |\n",
      "|Sandy|23 |Female|US     |\n",
      "+-----+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with a specified schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "dataSchema = StructType([       \n",
    "    StructField('Name', StringType(), False),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Country', StringType(), True)\n",
    "])\n",
    "\n",
    "df = ss.createDataFrame(people_json, schema = dataSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1cdc4-e2f4-4f07-9cbc-4d6491d759f4",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2148ed5d-1357-4160-ad58-8ec0355dbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| Name|Age|Country|\n",
      "+-----+---+-------+\n",
      "|Jacky| 27| Taiwan|\n",
      "| John| 32| Taiwan|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people from Taiwan and older than 25\n",
    "df.filter((df['Country'] == \"Taiwan\") & (df['Age'] > 25)) \\\n",
    "    .select(df['Name'], df['Age'], df['Country']) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a91fa74-e1ac-4cfd-8a5a-cce5e2da536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------+-------+-------+-----------+\n",
      "|Country|Count_Country|Min_Age|Max_Age|Sum_Age|Average_Age|\n",
      "+-------+-------------+-------+-------+-------+-----------+\n",
      "| Taiwan|            3|     16|     32|     75|       25.0|\n",
      "|     US|            1|     23|     23|     23|       23.0|\n",
      "|  Japan|            1|     42|     42|     42|       42.0|\n",
      "+-------+-------------+-------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy & where with functions\n",
    "from pyspark.sql.functions import col, sum, avg, max, min, count\n",
    "df.groupBy(\"Country\") \\\n",
    "  .agg(\n",
    "    count(\"Country\").alias(\"Count_Country\"),\n",
    "    min(\"Age\").alias(\"Min_Age\"),\n",
    "    max(\"Age\").alias(\"Max_Age\"),\n",
    "    sum(\"Age\").alias(\"Sum_Age\"),\n",
    "    avg(\"Age\").alias(\"Average_Age\")\n",
    "  ) \\\n",
    "  .where(col(\"Average_Age\") >= 10) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea90123a-4ecf-49df-ac5d-9276642424bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------+---------+---------+\n",
      "| Name|Age|Gender|Country| New Name|Power Age|\n",
      "+-----+---+------+-------+---------+---------+\n",
      "|Jacky| 27|  Male| Taiwan|NEW_Jacky|      729|\n",
      "| John| 32|  Male| Taiwan| NEW_John|     1024|\n",
      "|  Tom| 42|  Male|  Japan|  NEW_Tom|     1764|\n",
      "| Mark| 16|  Male| Taiwan| NEW_Mark|      256|\n",
      "|Sandy| 23|Female|     US|NEW_Sandy|      529|\n",
      "+-----+---+------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "class MyUdf:\n",
    "    def newname(s: str) -> str:\n",
    "        return f\"NEW_{s}\"\n",
    "    def powerage(i: int) -> int:\n",
    "        return i*i\n",
    "\n",
    "# Convert function to udf\n",
    "newname_udf = udf(lambda x: MyUdf.newname(x), StringType())\n",
    "powerage_udf = udf(lambda x: MyUdf.powerage(x), IntegerType())\n",
    "\n",
    "# Apply udf\n",
    "df1 = df.withColumn(\"New Name\", newname_udf(col(\"Name\")))\n",
    "df1 = df1.withColumn(\"Power Age\", powerage_udf(col(\"Age\")))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320a178d-b1e9-4d17-81a3-92e21375d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pyspark.pandas.frame.DataFrame'> \n",
      "     Name  Age Gender Country\n",
      "0  Jacky   27   Male  Taiwan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Country</th>\n",
       "      <th>New Name</th>\n",
       "      <th>Power Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jacky</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>NEW_Jacky</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John</td>\n",
       "      <td>32</td>\n",
       "      <td>Male</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>NEW_John</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NEW_Tom</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mark</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>NEW_Mark</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sandy</td>\n",
       "      <td>23</td>\n",
       "      <td>Female</td>\n",
       "      <td>US</td>\n",
       "      <td>NEW_Sandy</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age  Gender Country   New Name  Power Age\n",
       "0  Jacky   27    Male  Taiwan  NEW_Jacky        729\n",
       "1   John   32    Male  Taiwan   NEW_John       1024\n",
       "2    Tom   42    Male   Japan    NEW_Tom       1764\n",
       "3   Mark   16    Male  Taiwan   NEW_Mark        256\n",
       "4  Sandy   23  Female      US  NEW_Sandy        529"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark DataFrame doesn’t contain the apply() function \n",
    "# However, we can leverage Pandas DataFrame.apply() by running Pandas API over PySpark.\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = ps.DataFrame(df)\n",
    "print(f\"Type: {type(psdf)} \\n {psdf.head(1)}\")\n",
    "\n",
    "# Apply function to psdf\n",
    "psdf[\"New Name\"] = psdf[\"Name\"].apply(MyUdf.newname)\n",
    "psdf[\"Power Age\"] = psdf[\"Age\"].apply(MyUdf.powerage)\n",
    "\n",
    "# show\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eab5c401-ae5e-467a-b239-9a3a3d7c0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df.to_csv(\"./data/people.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716951f-cb32-490b-b134-e79bbdd573b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2186f-10b2-410f-b298-4d38677a8f23",
   "metadata": {},
   "source": [
    "## Create TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d6c27e-e2f1-4e35-b9af-64e71e911fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"PEOPLE_TABLE\")\n",
    "df.createOrReplaceGlobalTempView(\"GLOBAL_PEOPLE_TABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4889c5c-f805-43db-ada2-cd0c83cc9835",
   "metadata": {},
   "source": [
    "## SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b73bb8-47bb-41fd-bfd5-d1d377137d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------+\n",
      "| Name|Age|Gender|Country|\n",
      "+-----+---+------+-------+\n",
      "|Jacky| 27|  Male| Taiwan|\n",
      "| John| 32|  Male| Taiwan|\n",
      "|  Tom| 42|  Male|  Japan|\n",
      "| Mark| 16|  Male| Taiwan|\n",
      "|Sandy| 23|Female|     US|\n",
      "+-----+---+------+-------+\n",
      "\n",
      "+-----+---+------+-------+\n",
      "| Name|Age|Gender|Country|\n",
      "+-----+---+------+-------+\n",
      "|Jacky| 27|  Male| Taiwan|\n",
      "| John| 32|  Male| Taiwan|\n",
      "|  Tom| 42|  Male|  Japan|\n",
      "| Mark| 16|  Male| Taiwan|\n",
      "|Sandy| 23|Female|     US|\n",
      "+-----+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select from table / global table\n",
    "ss.sql(\"SELECT * FROM PEOPLE_TABLE\").show()\n",
    "ss.sql(\"SELECT * FROM global_temp.GLOBAL_PEOPLE_TABLE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c8e2f47-cb52-4f40-b918-2f11f3f36d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| Name|NEW_Name|\n",
      "+-----+--------+\n",
      "|Jacky|   JACKY|\n",
      "| John|    JOHN|\n",
      "|  Tom|     TOM|\n",
      "| Mark|    MARK|\n",
      "|Sandy|   SANDY|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql query\n",
    "ss.sql(\"\"\"\n",
    "        SELECT\n",
    "           Name, Upper(Name) AS NEW_Name\n",
    "        FROM\n",
    "           PEOPLE_TABLE\n",
    "       \"\"\"\n",
    "      ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c53b3df-051f-44f1-8767-155d7ba7663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+\n",
      "| Name|LOWER_Name|UPPER_Name|\n",
      "+-----+----------+----------+\n",
      "|Jacky|     jacky|     JACKY|\n",
      "| John|      john|      JOHN|\n",
      "|  Tom|       tom|       TOM|\n",
      "| Mark|      mark|      MARK|\n",
      "|Sandy|     sandy|     SANDY|\n",
      "+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql with UDF\n",
    "class MySqlUdf:\n",
    "    def lowercase(s: str) -> str:\n",
    "        return s.lower()\n",
    "    def uppercase(s: str) -> str:\n",
    "        return s.upper()\n",
    "\n",
    "# register\n",
    "ss.udf.register(\"lowercase_udf\", MySqlUdf.lowercase)\n",
    "ss.udf.register(\"uppercase_udf\", MySqlUdf.uppercase)\n",
    "\n",
    "# query\n",
    "ss.sql(\"\"\"\n",
    "        SELECT\n",
    "           Name, \n",
    "           lowercase_udf(Name) AS LOWER_Name,\n",
    "           uppercase_udf(Name) AS UPPER_Name\n",
    "        FROM\n",
    "           PEOPLE_TABLE\n",
    "       \"\"\"\n",
    "      ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4d1b9-1f23-4120-9b69-31710df1d08d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2202f1d-e7c0-48cc-b94f-854eda733898",
   "metadata": {},
   "source": [
    "### DataFrame-based API is primary API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bd0fb0d-f0bb-4825-8739-0786c0d5542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = ss.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "292961b4-52e8-402c-8bf1-a93b4b9aba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "# K-Means\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "# Need to pass absolute path, otherwise it willread from file://home/jovyan/...\n",
    "dataset = ss.read.format(\"libsvm\").load(\"file:///usr/local/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "103bf163-d9eb-49e9-adb8-1a1b8eedf8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.776502226798719\n",
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    20|[{22, 4.568694}, ...|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     20|[{17, 4.632046}, ...|\n",
      "+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    26|[{22, 5.149181}, ...|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     65|[{23, 4.6378446},...|\n",
      "+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CF (Collaborative Filtering)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Loads data.\n",
    "# Need to pass absolute path, otherwise it willread from file://home/jovyan/...\n",
    "lines = ss.read.text(\"file:///usr/local/spark/data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = ss.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10).show(1)\n",
    "\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10).show(1)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10).show(1)\n",
    "\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904a4a2-d40f-4352-ba3c-6038030618d7",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0722a1a-3952-4122-805d-cdbaa5517c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.6292098489668486,0.3707901510331514], prediction=0.000000\n",
      "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.13412348342566097,0.8658765165743391], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = ss.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = ss.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0422f5-6d67-4158-b76a-323feb482182",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573832b5-2210-4b3a-a9e5-9f9493fc0209",
   "metadata": {},
   "source": [
    "### Output Modes\n",
    "- Append mode (default): This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\n",
    "\n",
    "- Complete mode: The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\n",
    "\n",
    "- Update mode: (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb4aaf6a-d495-4930-bf5f-28a51b2e999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "isStreaming: True\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "dataSchema = StructType([       \n",
    "    StructField('Name', StringType(), False),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Country', StringType(), True)\n",
    "])\n",
    "\n",
    "# Read data stream from data folder\n",
    "# Trageting csv file and read to become a stream_df (Unbonded Table)\n",
    "stream_df = ss \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(dataSchema) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"./data\")\n",
    "\n",
    "# Check Streaming\n",
    "stream_df.printSchema()\n",
    "print(f\"isStreaming: {stream_df.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab8b420c-22a2-4d0e-8f72-4fc36fc7bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: Add column by using UDF\n",
    "stream_df = stream_df.withColumn(\"Power_Age\", powerage_udf(col(\"Age\")))\n",
    "\n",
    "# Transform: Aggregation operation\n",
    "stream_df = stream_df.groupBy(\"Country\") \\\n",
    "  .agg(\n",
    "    count(\"Country\").alias(\"Count_Country\"),\n",
    "    min(\"Age\").alias(\"Min_Age\"),\n",
    "    max(\"Age\").alias(\"Max_Age\"),\n",
    "    sum(\"Age\").alias(\"Sum_Age\"),\n",
    "    avg(\"Age\").alias(\"Average_Age\"),\n",
    "    avg(\"Power_Age\").alias(\"Average_Power_Age\")\n",
    "  ) \\\n",
    "  .where(col(\"Average_Age\") > 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2932093a-7be3-40e4-aa09-f2ea17b8a74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0xffff4a023250>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory Sink\n",
    "stream_df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"MY_TABLE\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37d38eef-7585-42d3-b4f9-c87ecdbd62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------+-------+-------+-----------+-----------------+\n",
      "|Country|Count_Country|Min_Age|Max_Age|Sum_Age|Average_Age|Average_Power_Age|\n",
      "+-------+-------------+-------+-------+-------+-----------+-----------------+\n",
      "| Taiwan|            3|     16|     32|     75|       25.0|669.6666666666666|\n",
      "|  Japan|            1|     42|     42|     42|       42.0|           1764.0|\n",
      "+-------+-------------+-------+-------+-------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show output (Might take some time to compute and display real output)\n",
    "ss.sql(\"select * from MY_TABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007835b9-baa5-49a4-b0da-0f7335bc4683",
   "metadata": {},
   "source": [
    "# Submit a spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c135295-911a-4a51-994b-25e9f8624d6f",
   "metadata": {},
   "source": [
    "- cd /usr/local/spark/bin/\n",
    "- spark-submit {MY-SPARK-CODE}.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b76502180bfafd82db37379610d66ecf1be7bea9c5a98ac727b707d9c027d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
